{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-harvest \u00b6 Store data created during your pytest tests execution, and retrieve it at the end of the session, e.g. for applicative benchmarking purposes. New simplified usage: with the default fixtures provided, you can now start collecting data from your tests in minutes ! pytest is a great tool to write test logic once and then generate multiple tests from parameters . Its fixture mechanism provides a cool way to inject dependencies in your tests. At the end of a test session, you can already collect various data about the tests that have been run. But it is a bit cumbersome to get it right, and requires you to write a plugin (see this advice ). Besides, as opposed to parameters ( @pytest.mark.parametrize ), pytest purposedly does not keep fixtures ( @pytest.fixture ) in memory, because in general that would just be a waste of memory. Therefore you are currently not able to retrieve fixture values at the end of the session . Finally, what about other kind of results that you produce during test execution ? There is no current mechanism in pytest to manage that. With pytest-harvest : you can store all instances of a fixture with @saved_fixture , so that they remain available until the end of the test session. If you're only interested in some aspects of the fixture, you can store \" views \" instead. you can use the special results_bag fixture to collect interesting results within your tests. you can use the special [session/module]_results_[dct/df] fixtures to easily collect all available data at the end of a session or module, without having to register pytest hooks. The status, duration and parameters of all tests become easily available both as dictionary or pandas dataframe, and your saved fixtures and results are there too. you can create your own variants of the above thanks to the API, for more customized data collection and synthesis. With all that, you can now easily create applicative benchmarks . See pytest-patterns for an example of data science benchmark. Note pytest-harvest has not yet been tested with pytest-xdist. See #1 Installing \u00b6 > pip install pytest_harvest Usage \u00b6 a- Storing fixture instances \u00b6 Simply use the @saved_fixture decorator on your fixtures to declare that their instances must be saved. By default they are saved in a session-scoped fixture_store fixture that you can therefore grab and inspect in other tests or in any compliant pytest entry point: import pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = range ( 2 )) @saved_fixture def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" def test_foo ( person ): \"\"\" A dummy test, executed for each `person` fixture available \"\"\" print ( ' \\n hello, ' + person + ' !' ) def test_synthesis ( fixture_store ): \"\"\" In this test we inspect the contents of the fixture store so far, and check that the 'person' entry contains a dict <test_id>: <person> \"\"\" # print the keys in the store print ( \" \\n Available `fixture_store` keys:\" ) for k in fixture_store : print ( \" - ' %s '\" % k ) # print what is available for the 'person' entry print ( \" \\n Contents of `fixture_store['person']`:\" ) for k , v in fixture_store [ 'person' ] . items (): print ( \" - ' %s ': %s \" % ( k , v )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic_saved_fixture.py::test_foo [ 0 ] hello, world ! PASSED test_doc_basic_saved_fixture.py::test_foo [ 1 ] hello, self ! PASSED test_doc_basic_saved_fixture.py::test_synthesis Available ` fixture_store ` keys: - 'person' Contents of ` fixture_store [ 'person' ] ` : - 'test_doc_basic_saved_fixture.py::test_foo[0]' : world - 'test_doc_basic_saved_fixture.py::test_foo[1]' : self PASSED ========================== 3 passed in 0 .09 seconds =========================== As you can see, the fixture_store contains one entry for each saved fixture, and this entry's value is a dictionary of {<test_id>: <fixture_value>} . We will see below how to combine this information with information already available in pytest (test status, duration...). Storing fixture views \u00b6 Sometimes you are not interested in storing the whole fixture but maybe just some aspect of it. For example maybe the fixture is a huge dataset, and you just wish to remember a few characteristics about it. Simply use the views= argument in @saved_fixture to save views instead of the fixture itself. That argument should contain a dictionary of {<view_key>: <view_creation_function>} . In the previous example if we only want to save the first and last character of the person fixture, we can do: @pytest.fixture ( params = range ( 2 )) @saved_fixture ( views = { 'person_initial' : lambda p : p [ 0 ], 'person_last_char' : lambda p : p [ - 1 ]}) def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" The fixture store will then contain as many entries as there are views. b- Collecting test artifacts \u00b6 Simply use the results_bag fixture in your tests and you'll be able to store items in it. This object behaves like a munch: if you create/read a field it will create/read a dictionary entry. By default the results_bag fixture is stored in the fixture_store so you can retrieve it at the end as shown previously. from datetime import datetime import pytest @pytest.mark.parametrize ( 'p' , [ 'world' , 'self' ], ids = str ) def test_foo ( p , results_bag ): \"\"\" A dummy test, parametrized so that it is executed twice \"\"\" print ( ' \\n hello, ' + p + ' !' ) # Let's store some things in the results bag results_bag . nb_letters = len ( p ) results_bag . current_time = datetime . now () . isoformat () def test_synthesis ( fixture_store ): \"\"\" In this test we inspect the contents of the fixture store so far, and check that the 'results_bag' entry contains a dict <test_id>: <results_bag> \"\"\" # print the keys in the store print ( \" \\n Available `fixture_store` keys:\" ) for k in fixture_store : print ( \" - ' %s '\" % k ) # print what is available for the 'results_bag' entry print ( \" \\n Contents of `fixture_store['results_bag']`:\" ) for k , v in fixture_store [ 'results_bag' ] . items (): print ( \" - ' %s ':\" % k ) for kk , vv in v . items (): print ( \" - ' %s ': %s \" % ( kk , vv )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic_results_bag.py::test_foo [ world ] hello, world ! PASSED test_doc_basic_results_bag.py::test_foo [ self ] hello, self ! PASSED test_doc_basic_results_bag.py::test_synthesis Available ` fixture_store ` keys: - 'results_bag' Contents of ` fixture_store [ 'results_bag' ] ` : - 'test_doc_basic_results_bag.py::test_foo[world]' : - 'nb_letters' : 5 - 'current_time' : 2018 -12-08T22:20:10.695791 - 'test_doc_basic_results_bag.py::test_foo[self]' : - 'nb_letters' : 4 - 'current_time' : 2018 -12-08T22:20:10.700791 PASSED ========================== 3 passed in 0 .05 seconds =========================== As in previous example, the fixture_store contains one entry for 'results_bag' , and this entry's value is a dictionary of {<test_id>: <results_bag>} . We can therefore access all values stored within each test (here, nb_letters and current_time ). We will see below how to combine this information with information already available in pytest. c- Collecting a synthesis \u00b6 as a dict Simply use the module_results_dct fixture to get a dictionary containing the test results in that module , so far . You can use this fixture in a test as shown below ( test_synthesis ) or in any compliant pytest entry point. import pytest import time @pytest.mark.parametrize ( 'p' , [ 'world' , 'self' ], ids = str ) def test_foo ( p ): \"\"\" A dummy test, parametrized so that it is executed twice \"\"\" print ( ' \\n hello, ' + p + ' !' ) time . sleep ( len ( p ) / 10 ) def test_synthesis ( module_results_dct ): \"\"\" In this test we just look at the synthesis of all tests executed before it, in that module. \"\"\" # print the keys in the synthesis dictionary print ( \" \\n Available `module_results_dct` keys:\" ) for k in module_results_dct : print ( \" - \" + k ) # print what is available for a single test print ( \" \\n Contents of 'test_foo[world]':\" ) for k , v in module_results_dct [ 'test_foo[world]' ] . items (): if k != 'status_details' : print ( \" - ' %s ': %s \" % ( k , v )) else : print ( \" - ' %s ':\" % k ) for kk , vv in v . items (): print ( \" - ' %s ': %s \" % ( kk , vv )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic.py::test_foo [ world ] hello, world ! PASSED test_doc_basic.py::test_foo [ self ] hello, self ! PASSED test_doc_basic.py::test_synthesis Available ` module_results_dct ` keys: - test_foo [ world ] - test_foo [ self ] Contents of 'test_foo[world]' : - 'pytest_obj' : < function test_foo at 0x0000000005A7DEA0> - 'status' : passed - 'duration_ms' : 500 .0283718109131 - 'status_details' : - 'setup' : ( 'passed' , 3 .0002593994140625 ) - 'call' : ( 'passed' , 500 .0283718109131 ) - 'teardown' : ( 'passed' , 2 .0003318786621094 ) - 'params' : OrderedDict ([( 'p' , 'world' )]) - 'fixtures' : OrderedDict () PASSED ========================== 3 passed in 0 .05 seconds =========================== As you can see, for each test node id you get a dictionary containing 'pytest_obj' the object containing the test code 'status' the status of the test (passed/skipped/failed) 'duration_ms' the duration of the test as measured by pytest (only the \"call\" step is measured here, not setup nor teardown times) 'status_details' : details (status and duration) for each pytest phase 'params' the parameters used in this test (both in the test function AND the fixtures) 'fixtures' the saved fixture instances (not parameters) for this test. Here we see the saved fixtures and result bags, if any (see below for a complete example) Note: if you need the synthesis to contain all tests of the session instead of just the current module , use fixtures session_results_dct instead. as a DataFrame Simply use the module_results_df fixture instead of module_results_dct (note the df suffix instead of dct ) to get the same contents as a table, which might be more convenient for statistics and aggregations of all sorts. Note: you have to have pandas installed for this fixture to be available. Replacing the above test_synthesis function with def test_synthesis ( module_results_df ): \"\"\" In this variant we use the 'dataframe' fixture \"\"\" # print the synthesis dataframe print ( \" \\n `module_results_df` dataframe: \\n \" ) print ( module_results_df ) yields: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic.py::test_foo [ world ] hello, world ! PASSED test_doc_basic.py::test_foo [ self ] hello, self ! PASSED test_doc_basic.py::test_synthesis ` module_results_df ` dataframe: status duration_ms p test_id test_foo [ world ] passed 500 .028610 world test_foo [ self ] passed 400 .022745 self PASSED ========================== 3 passed in 0 .05 seconds =========================== As can be seen above, each row in the dataframe corresponds to a test (the index is the test id), and the various information are presented in columns. As opposed to the dictionary version, status details are not provided. Note: as for the dict version, if you need the synthesis to contain all tests of the session instead of just the current module , use fixtures session_results_df instead. d- collecting all at once \u00b6 We have seen first how to collect saved fixtures , and test artifacts thanks to results bags. Then we saw how to collect pytest status and duration information, as well as parameters . You may now wonder how to collect all of this in a single handy object ? Well, the answer is quite simple: you have nothing more to do. Indeed, the [module/session]_results_[dct/df] fixtures that we saw in previous chapter will by default contain all saved fixtures and results bags. Let's try it: import time from datetime import datetime from tabulate import tabulate import pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = range ( 2 )) @saved_fixture def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" @pytest.mark.parametrize ( 'double_sleep_time' , [ False , True ], ids = str ) def test_foo ( double_sleep_time , person , results_bag ): \"\"\" A dummy test, parametrized so that it is executed twice. \"\"\" print ( ' \\n hello, ' + person + ' !' ) time . sleep ( len ( person ) / 10 * ( 2 if double_sleep_time else 1 )) # Let's store some things in the results bag results_bag . nb_letters = len ( person ) results_bag . current_time = datetime . now () . isoformat () def test_synthesis ( module_results_df ): \"\"\" In this test we just look at the synthesis of all tests executed before it, in that module. \"\"\" # print the synthesis dataframe print ( \" \\n `module_results_df` dataframe: \\n \" ) # we use 'tabulate' for a nicer output format print ( tabulate ( module_results_df , headers = 'keys' , tablefmt = \"pipe\" )) yields >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 5 items test_doc_basic_df_all.py::test_foo [ 0 -False ] test_doc_basic_df_all.py::test_foo [ 0 -True ] test_doc_basic_df_all.py::test_foo [ 1 -False ] test_doc_basic_df_all.py::test_foo [ 1 -True ] test_doc_basic_df_all.py::test_synthesis hello, world ! PASSED hello, world ! PASSED hello, self ! PASSED hello, self ! PASSED ` module_results_df ` dataframe: | test_id | pytest_obj | status | duration_ms | double_sleep_time | person_param | person | nb_letters | current_time | | :------------------ | :------------------------------------------ | :--------- | --------------: | :-------------------- | ---------------: | :--------- | -------------: | :--------------------------- | | test_foo [ 0 -False ] | < function test_foo at 0x0000000004F8C488> | passed | 500 .029 | False | 0 | world | 5 | 2018 -12-10T22:06:32.279561 | | test_foo [ 0 -True ] | < function test_foo at 0x0000000004F8C488> | passed | 1000 .06 | True | 0 | world | 5 | 2018 -12-10T22:06:33.283618 | | test_foo [ 1 -False ] | < function test_foo at 0x0000000004F8C488> | passed | 400 .023 | False | 1 | self | 4 | 2018 -12-10T22:06:33.687641 | | test_foo [ 1 -True ] | < function test_foo at 0x0000000004F8C488> | passed | 800 .046 | True | 1 | self | 4 | 2018 -12-10T22:06:34.491687 | PASSED ========================== 5 passed in 3 .87 seconds =========================== So we see here that we get all the information in a single handy table object: for each test, we get its status, duration, parameters ( double_sleep_time , person_param ), fixtures ( person ) and results ( nb_letters , current_time ). Of course you can still get the same information as a dictionary, and chose to get it for the whole session or for a specific module (see previous chapter ). e- customization \u00b6 All the behaviours described above are pre-wired to help most users getting started. However they are nothing but pre-wiring of more generic capabilities, that are offered in this library as well. For details on how to create custom synthesis , custom store objects , custom results bags ... see advanced usage page . Compliance with the pytest ecosystem \u00b6 This plugin mostly relies on the fixtures mechanism and the pytest_runtest_makereport hook. It should therefore be quite portable across pytest versions (at least it is tested against pytest 2 and 3, for both python 2 and 3). Main features / benefits \u00b6 Collect test execution information easily : with the default [module/session]_results_[dct/df] fixtures, and with get_session_synthesis_dct(session) (advanced users), you can collect all the information you need, without the hassle of writing hooks. Store selected fixtures declaratively : simply decorate your fixture with @saved_fixture and all fixture values will be stored in the default storage. You can use the advanced @saved_fixture(store) to customize the storage (a variable or another fixture). Collect test artifacts : simply use the results_bag fixture to start collecting results from your tests. You can also create your own \"results bags\" fixtures (advanced). It makes it very easy to create applicative benchmarks, for example for data science . Highly configurable : storage object (for storing fixtures) or results bag objects (for collecting results from tests) can be of any object type of your choice. For results bags, a default type is provided that behaves like a \"munch\" (both a dictionary and an object). See advanced usage page . See Also \u00b6 pytest documentation on parametrize pytest documentation on fixtures pytest-patterns , to go further and create for example a data science benchmark by combining this plugin with others. Others \u00b6 Do you like this library ? You might also like my other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/smarie/python-pytest-harvest","title":"Home"},{"location":"#pytest-harvest","text":"Store data created during your pytest tests execution, and retrieve it at the end of the session, e.g. for applicative benchmarking purposes. New simplified usage: with the default fixtures provided, you can now start collecting data from your tests in minutes ! pytest is a great tool to write test logic once and then generate multiple tests from parameters . Its fixture mechanism provides a cool way to inject dependencies in your tests. At the end of a test session, you can already collect various data about the tests that have been run. But it is a bit cumbersome to get it right, and requires you to write a plugin (see this advice ). Besides, as opposed to parameters ( @pytest.mark.parametrize ), pytest purposedly does not keep fixtures ( @pytest.fixture ) in memory, because in general that would just be a waste of memory. Therefore you are currently not able to retrieve fixture values at the end of the session . Finally, what about other kind of results that you produce during test execution ? There is no current mechanism in pytest to manage that. With pytest-harvest : you can store all instances of a fixture with @saved_fixture , so that they remain available until the end of the test session. If you're only interested in some aspects of the fixture, you can store \" views \" instead. you can use the special results_bag fixture to collect interesting results within your tests. you can use the special [session/module]_results_[dct/df] fixtures to easily collect all available data at the end of a session or module, without having to register pytest hooks. The status, duration and parameters of all tests become easily available both as dictionary or pandas dataframe, and your saved fixtures and results are there too. you can create your own variants of the above thanks to the API, for more customized data collection and synthesis. With all that, you can now easily create applicative benchmarks . See pytest-patterns for an example of data science benchmark. Note pytest-harvest has not yet been tested with pytest-xdist. See #1","title":"pytest-harvest"},{"location":"#installing","text":"> pip install pytest_harvest","title":"Installing"},{"location":"#usage","text":"","title":"Usage"},{"location":"#a-storing-fixture-instances","text":"Simply use the @saved_fixture decorator on your fixtures to declare that their instances must be saved. By default they are saved in a session-scoped fixture_store fixture that you can therefore grab and inspect in other tests or in any compliant pytest entry point: import pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = range ( 2 )) @saved_fixture def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" def test_foo ( person ): \"\"\" A dummy test, executed for each `person` fixture available \"\"\" print ( ' \\n hello, ' + person + ' !' ) def test_synthesis ( fixture_store ): \"\"\" In this test we inspect the contents of the fixture store so far, and check that the 'person' entry contains a dict <test_id>: <person> \"\"\" # print the keys in the store print ( \" \\n Available `fixture_store` keys:\" ) for k in fixture_store : print ( \" - ' %s '\" % k ) # print what is available for the 'person' entry print ( \" \\n Contents of `fixture_store['person']`:\" ) for k , v in fixture_store [ 'person' ] . items (): print ( \" - ' %s ': %s \" % ( k , v )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic_saved_fixture.py::test_foo [ 0 ] hello, world ! PASSED test_doc_basic_saved_fixture.py::test_foo [ 1 ] hello, self ! PASSED test_doc_basic_saved_fixture.py::test_synthesis Available ` fixture_store ` keys: - 'person' Contents of ` fixture_store [ 'person' ] ` : - 'test_doc_basic_saved_fixture.py::test_foo[0]' : world - 'test_doc_basic_saved_fixture.py::test_foo[1]' : self PASSED ========================== 3 passed in 0 .09 seconds =========================== As you can see, the fixture_store contains one entry for each saved fixture, and this entry's value is a dictionary of {<test_id>: <fixture_value>} . We will see below how to combine this information with information already available in pytest (test status, duration...).","title":"a- Storing fixture instances"},{"location":"#storing-fixture-views","text":"Sometimes you are not interested in storing the whole fixture but maybe just some aspect of it. For example maybe the fixture is a huge dataset, and you just wish to remember a few characteristics about it. Simply use the views= argument in @saved_fixture to save views instead of the fixture itself. That argument should contain a dictionary of {<view_key>: <view_creation_function>} . In the previous example if we only want to save the first and last character of the person fixture, we can do: @pytest.fixture ( params = range ( 2 )) @saved_fixture ( views = { 'person_initial' : lambda p : p [ 0 ], 'person_last_char' : lambda p : p [ - 1 ]}) def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" The fixture store will then contain as many entries as there are views.","title":"Storing fixture views"},{"location":"#b-collecting-test-artifacts","text":"Simply use the results_bag fixture in your tests and you'll be able to store items in it. This object behaves like a munch: if you create/read a field it will create/read a dictionary entry. By default the results_bag fixture is stored in the fixture_store so you can retrieve it at the end as shown previously. from datetime import datetime import pytest @pytest.mark.parametrize ( 'p' , [ 'world' , 'self' ], ids = str ) def test_foo ( p , results_bag ): \"\"\" A dummy test, parametrized so that it is executed twice \"\"\" print ( ' \\n hello, ' + p + ' !' ) # Let's store some things in the results bag results_bag . nb_letters = len ( p ) results_bag . current_time = datetime . now () . isoformat () def test_synthesis ( fixture_store ): \"\"\" In this test we inspect the contents of the fixture store so far, and check that the 'results_bag' entry contains a dict <test_id>: <results_bag> \"\"\" # print the keys in the store print ( \" \\n Available `fixture_store` keys:\" ) for k in fixture_store : print ( \" - ' %s '\" % k ) # print what is available for the 'results_bag' entry print ( \" \\n Contents of `fixture_store['results_bag']`:\" ) for k , v in fixture_store [ 'results_bag' ] . items (): print ( \" - ' %s ':\" % k ) for kk , vv in v . items (): print ( \" - ' %s ': %s \" % ( kk , vv )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic_results_bag.py::test_foo [ world ] hello, world ! PASSED test_doc_basic_results_bag.py::test_foo [ self ] hello, self ! PASSED test_doc_basic_results_bag.py::test_synthesis Available ` fixture_store ` keys: - 'results_bag' Contents of ` fixture_store [ 'results_bag' ] ` : - 'test_doc_basic_results_bag.py::test_foo[world]' : - 'nb_letters' : 5 - 'current_time' : 2018 -12-08T22:20:10.695791 - 'test_doc_basic_results_bag.py::test_foo[self]' : - 'nb_letters' : 4 - 'current_time' : 2018 -12-08T22:20:10.700791 PASSED ========================== 3 passed in 0 .05 seconds =========================== As in previous example, the fixture_store contains one entry for 'results_bag' , and this entry's value is a dictionary of {<test_id>: <results_bag>} . We can therefore access all values stored within each test (here, nb_letters and current_time ). We will see below how to combine this information with information already available in pytest.","title":"b- Collecting test artifacts"},{"location":"#c-collecting-a-synthesis","text":"as a dict Simply use the module_results_dct fixture to get a dictionary containing the test results in that module , so far . You can use this fixture in a test as shown below ( test_synthesis ) or in any compliant pytest entry point. import pytest import time @pytest.mark.parametrize ( 'p' , [ 'world' , 'self' ], ids = str ) def test_foo ( p ): \"\"\" A dummy test, parametrized so that it is executed twice \"\"\" print ( ' \\n hello, ' + p + ' !' ) time . sleep ( len ( p ) / 10 ) def test_synthesis ( module_results_dct ): \"\"\" In this test we just look at the synthesis of all tests executed before it, in that module. \"\"\" # print the keys in the synthesis dictionary print ( \" \\n Available `module_results_dct` keys:\" ) for k in module_results_dct : print ( \" - \" + k ) # print what is available for a single test print ( \" \\n Contents of 'test_foo[world]':\" ) for k , v in module_results_dct [ 'test_foo[world]' ] . items (): if k != 'status_details' : print ( \" - ' %s ': %s \" % ( k , v )) else : print ( \" - ' %s ':\" % k ) for kk , vv in v . items (): print ( \" - ' %s ': %s \" % ( kk , vv )) Let's execute it: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic.py::test_foo [ world ] hello, world ! PASSED test_doc_basic.py::test_foo [ self ] hello, self ! PASSED test_doc_basic.py::test_synthesis Available ` module_results_dct ` keys: - test_foo [ world ] - test_foo [ self ] Contents of 'test_foo[world]' : - 'pytest_obj' : < function test_foo at 0x0000000005A7DEA0> - 'status' : passed - 'duration_ms' : 500 .0283718109131 - 'status_details' : - 'setup' : ( 'passed' , 3 .0002593994140625 ) - 'call' : ( 'passed' , 500 .0283718109131 ) - 'teardown' : ( 'passed' , 2 .0003318786621094 ) - 'params' : OrderedDict ([( 'p' , 'world' )]) - 'fixtures' : OrderedDict () PASSED ========================== 3 passed in 0 .05 seconds =========================== As you can see, for each test node id you get a dictionary containing 'pytest_obj' the object containing the test code 'status' the status of the test (passed/skipped/failed) 'duration_ms' the duration of the test as measured by pytest (only the \"call\" step is measured here, not setup nor teardown times) 'status_details' : details (status and duration) for each pytest phase 'params' the parameters used in this test (both in the test function AND the fixtures) 'fixtures' the saved fixture instances (not parameters) for this test. Here we see the saved fixtures and result bags, if any (see below for a complete example) Note: if you need the synthesis to contain all tests of the session instead of just the current module , use fixtures session_results_dct instead. as a DataFrame Simply use the module_results_df fixture instead of module_results_dct (note the df suffix instead of dct ) to get the same contents as a table, which might be more convenient for statistics and aggregations of all sorts. Note: you have to have pandas installed for this fixture to be available. Replacing the above test_synthesis function with def test_synthesis ( module_results_df ): \"\"\" In this variant we use the 'dataframe' fixture \"\"\" # print the synthesis dataframe print ( \" \\n `module_results_df` dataframe: \\n \" ) print ( module_results_df ) yields: >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 3 items test_doc_basic.py::test_foo [ world ] hello, world ! PASSED test_doc_basic.py::test_foo [ self ] hello, self ! PASSED test_doc_basic.py::test_synthesis ` module_results_df ` dataframe: status duration_ms p test_id test_foo [ world ] passed 500 .028610 world test_foo [ self ] passed 400 .022745 self PASSED ========================== 3 passed in 0 .05 seconds =========================== As can be seen above, each row in the dataframe corresponds to a test (the index is the test id), and the various information are presented in columns. As opposed to the dictionary version, status details are not provided. Note: as for the dict version, if you need the synthesis to contain all tests of the session instead of just the current module , use fixtures session_results_df instead.","title":"c- Collecting a synthesis"},{"location":"#d-collecting-all-at-once","text":"We have seen first how to collect saved fixtures , and test artifacts thanks to results bags. Then we saw how to collect pytest status and duration information, as well as parameters . You may now wonder how to collect all of this in a single handy object ? Well, the answer is quite simple: you have nothing more to do. Indeed, the [module/session]_results_[dct/df] fixtures that we saw in previous chapter will by default contain all saved fixtures and results bags. Let's try it: import time from datetime import datetime from tabulate import tabulate import pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = range ( 2 )) @saved_fixture def person ( request ): \"\"\" A dummy fixture, parametrized so that it has two instances \"\"\" if request . param == 0 : return \"world\" elif request . param == 1 : return \"self\" @pytest.mark.parametrize ( 'double_sleep_time' , [ False , True ], ids = str ) def test_foo ( double_sleep_time , person , results_bag ): \"\"\" A dummy test, parametrized so that it is executed twice. \"\"\" print ( ' \\n hello, ' + person + ' !' ) time . sleep ( len ( person ) / 10 * ( 2 if double_sleep_time else 1 )) # Let's store some things in the results bag results_bag . nb_letters = len ( person ) results_bag . current_time = datetime . now () . isoformat () def test_synthesis ( module_results_df ): \"\"\" In this test we just look at the synthesis of all tests executed before it, in that module. \"\"\" # print the synthesis dataframe print ( \" \\n `module_results_df` dataframe: \\n \" ) # we use 'tabulate' for a nicer output format print ( tabulate ( module_results_df , headers = 'keys' , tablefmt = \"pipe\" )) yields >>> pytest -s -v ============================= test session starts ============================= ... collecting ... collected 5 items test_doc_basic_df_all.py::test_foo [ 0 -False ] test_doc_basic_df_all.py::test_foo [ 0 -True ] test_doc_basic_df_all.py::test_foo [ 1 -False ] test_doc_basic_df_all.py::test_foo [ 1 -True ] test_doc_basic_df_all.py::test_synthesis hello, world ! PASSED hello, world ! PASSED hello, self ! PASSED hello, self ! PASSED ` module_results_df ` dataframe: | test_id | pytest_obj | status | duration_ms | double_sleep_time | person_param | person | nb_letters | current_time | | :------------------ | :------------------------------------------ | :--------- | --------------: | :-------------------- | ---------------: | :--------- | -------------: | :--------------------------- | | test_foo [ 0 -False ] | < function test_foo at 0x0000000004F8C488> | passed | 500 .029 | False | 0 | world | 5 | 2018 -12-10T22:06:32.279561 | | test_foo [ 0 -True ] | < function test_foo at 0x0000000004F8C488> | passed | 1000 .06 | True | 0 | world | 5 | 2018 -12-10T22:06:33.283618 | | test_foo [ 1 -False ] | < function test_foo at 0x0000000004F8C488> | passed | 400 .023 | False | 1 | self | 4 | 2018 -12-10T22:06:33.687641 | | test_foo [ 1 -True ] | < function test_foo at 0x0000000004F8C488> | passed | 800 .046 | True | 1 | self | 4 | 2018 -12-10T22:06:34.491687 | PASSED ========================== 5 passed in 3 .87 seconds =========================== So we see here that we get all the information in a single handy table object: for each test, we get its status, duration, parameters ( double_sleep_time , person_param ), fixtures ( person ) and results ( nb_letters , current_time ). Of course you can still get the same information as a dictionary, and chose to get it for the whole session or for a specific module (see previous chapter ).","title":"d- collecting all at once"},{"location":"#e-customization","text":"All the behaviours described above are pre-wired to help most users getting started. However they are nothing but pre-wiring of more generic capabilities, that are offered in this library as well. For details on how to create custom synthesis , custom store objects , custom results bags ... see advanced usage page .","title":"e- customization"},{"location":"#compliance-with-the-pytest-ecosystem","text":"This plugin mostly relies on the fixtures mechanism and the pytest_runtest_makereport hook. It should therefore be quite portable across pytest versions (at least it is tested against pytest 2 and 3, for both python 2 and 3).","title":"Compliance with the pytest ecosystem"},{"location":"#main-features-benefits","text":"Collect test execution information easily : with the default [module/session]_results_[dct/df] fixtures, and with get_session_synthesis_dct(session) (advanced users), you can collect all the information you need, without the hassle of writing hooks. Store selected fixtures declaratively : simply decorate your fixture with @saved_fixture and all fixture values will be stored in the default storage. You can use the advanced @saved_fixture(store) to customize the storage (a variable or another fixture). Collect test artifacts : simply use the results_bag fixture to start collecting results from your tests. You can also create your own \"results bags\" fixtures (advanced). It makes it very easy to create applicative benchmarks, for example for data science . Highly configurable : storage object (for storing fixtures) or results bag objects (for collecting results from tests) can be of any object type of your choice. For results bags, a default type is provided that behaves like a \"munch\" (both a dictionary and an object). See advanced usage page .","title":"Main features / benefits"},{"location":"#see-also","text":"pytest documentation on parametrize pytest documentation on fixtures pytest-patterns , to go further and create for example a data science benchmark by combining this plugin with others.","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like my other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/smarie/python-pytest-harvest","title":"Want to contribute ?"},{"location":"advanced_usage/","text":"Advanced Usage \u00b6 You have seen in the introduction how to use the pre-wired fixtures to perform most comon tasks. The sections below explore, for each topic, the internals that are used behind the scenes, so that you are able to create custom behaviours if the default ones do not match your needs. 0- Prerequisite: how to write session teardown code \u00b6 In order to be able to retrieve all the information that we will store, we will need to execute our retrieval/synthesis code at the end of the entire test session . pytest currently provides several ways to do this: through a test (put it at the end of the latest test file to ensure execution at the end): def test_synthesis ( request ): # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) through a generator (yield) session-scoped fixture (put this in any of your test files or in the conftest.py file): # Note: for pytest<3.0 you have to use @pytest.yield_fixture instead @pytest.fixture ( scope = 'session' , autouse = True ) def my_cooler_session_finish ( request ): yield # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) through a normal session-scoped fixture (put this in any of your test files or in the conftest.py file): @pytest.fixture ( scope = \"session\" , autouse = True ) def my_session_finish ( request ): def _end (): # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) request . addfinalizer ( _end ) through the session finish hook (you have to write this function in the conftest.py file): def pytest_sessionfinish ( session , exitstatus ): print ( \"<Put here your synthesis code>\" ) All seem completely equivalent for the usage of pytest_harvest . I personally prefer the \"fixture\" style because you can have several of them instead of a monolithic teardown hook. Besides they can be put in the test files so I typically put them as close as possible to the tests that store the data, so as to ensure maintainability (data creation/storage and data retrieval/synthesis code are in the same file). 1- Collecting tests status and parameters \u00b6 Pytest already stores some information out of the box concerning tests that have run. In addition you can follow this example to retrieve more, but it requires you to write a hook so it is not very convenient. Instead, you can easily retrieve all of that thanks to the get_session_synthesis_dct(session) utility function. For example let's assume you have this parametrized test with a parametrized fixture: # unparametrized fixture @pytest.fixture def dummy (): return \"hey there !\" # parametrized fixture @pytest.fixture ( param = [ 1 , 2 ]) def a_number_str ( request ): return \"my_fix # %s \" % request . param # parametrized test using the fixtures @pytest.mark.parametrize ( 'p' , [ 'hello' , 'world' ], ids = str ) def test_foo ( p , a_number_str , dummy ): print ( p + a_number_str ) When running it, 4 tests are executed: >>> pytest ============================= test session starts ============================= collected 4 items path/to/test_file.py::test_foo [ 1 -hello ] PASSED [ 25 % ] path/to/test_file.py::test_foo [ 1 -world ] PASSED [ 50 % ] path/to/test_file.py::test_foo [ 2 -hello ] PASSED [ 75 % ] path/to/test_file.py::test_foo [ 2 -world ] PASSED [ 100 % ] ========================== 4 passed in 0 .11 seconds =========================== let's retrieve the available information at the end of the session. The easiest way is to write a \"last test\" and make sure it is executed after all the other as shown below, but there are other ways as shown above : from pytest_harvest import get_session_synthesis_dct def test_synthesis ( request ): synth_dct = get_session_synthesis_dct ( request . session , status_details = True ) print ( dict ( synth_dct )) It yields { 'path/to/test_file.py::test_foo[1-hello]' : { 'pytest_duration' : 0.0010001659393310547 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 1 , 'p' : 'hello' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0010001659393310547 ), 'setup' : ( 'passed' , 0.013001203536987305 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[1-world]' : { 'pytest_duration' : 0.0 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 1 , 'p' : 'world' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0 ), 'setup' : ( 'passed' , 0.0 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[2-hello]' : { 'pytest_duration' : 0.0010001659393310547 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 2 , 'p' : 'hello' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0010001659393310547 ), 'setup' : ( 'passed' , 0.0010001659393310547 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[2-world]' : { 'pytest_duration' : 0.0 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 2 , 'p' : 'world' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0 ), 'setup' : ( 'passed' , 0.0010001659393310547 ), 'teardown' : ( 'passed' , 0.0 ) } } } As you can see for each test node id you get a dictionary containing 'pytest_obj' the object containing the test code 'pytest_status' the status and 'pytest_duration' the duration of the test 'pytest_params' the parameters used in this test (both in the test function AND the fixture) 'pytest_status_details' a dictionary containing the status details for each pytest internal stages: setup, call, and teardown. status and duration aggregation that the global status corresponds to an aggregation of the status of each of those stages (setup, call, teardown), but the global duration is only the duration of the \"call\" stage - after all we do not care about how long it took to setup and teardown. In addition, you can also use the following companion methods : filter_session_items(session, filter=None) is the filtering method used behind the scenes. pytest_item_matches_filter is the inner method used to test if a single item matches the filter. get_all_pytest_param_names(session) lists all unique parameter names used, with optional filtering capabilities is_pytest_incomplete(item) , get_pytest_status(item) , get_pytest_param_names(item) and get_pytest_params(item) can be used to analyse a specific item in session.items directly without creating the dictionary. Finally let's have a closer look above. It seems that after all we have collected the fixtures, right ? For example we see 'a_number_str_param': 2 . But beware, this is not the fixture. It is the parameter used by the fixture 'a_number_str' . To be convinced look at its type: it is an integer, not a string! A more obvious way to confirm that the fixtures are not available, is to see that the 'dummy' fixture does not appear at all: indeed it had no parameters. To conclude: the get_session_synthesis_dct(session) utility function allows you to collect many information about the tests, but not the fixtures. To do that you have to use the mechanisms below. 2- Storing/retrieving fixtures \u00b6 You can either choose to store fixtures in a plain old variable, or in another, session-scoped, fixture. Both can be achieved using the same decorator @saved_fixture(store) . a- Storing in a variable \u00b6 Let's create a store. It should be a dict-like object: # Create a global store STORE = dict () In order to store all created fixture values in this object, simply decorate your fixture with @saved_fixture(STORE) : from pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = [ 1 , 2 ]) @saved_fixture ( STORE ) def my_fix ( request ): \"\"\"Each returned fixture value will be saved in the global store\"\"\" return \"my_fix # %s \" % request . param You can then retrieve the available information at the end of the session (put this in your session teardown ): print ( dict ( STORE [ 'my_fix' ])) Each saved fixture appears as an ordered dictionary stored under a global key that has its name (here 'my_fix'). In this dictionary, the keys are the test node ids, and the values are the fixture values: { 'path/to/test_file.py::test_foo[1]' : 'my_fix #1' , 'path/to/test_file.py::test_foo[2]' : 'my_fix #2' } Note: you can change the key used in the global storage with the key= argument of @saved_fixture . b- Storing in a fixture \u00b6 You might want your store to be a fixture itself, instead of a global variable. It is possible if it is session- or module-scoped (in the same module than where it is used). Simply use its name in @saved_fixture and it will work as expected. This enables you to make the code even more readable because you can put the synthesis code in the teardown part of the storage fixture: from pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = [ 1 , 2 ]) @saved_fixture ( \"store\" ) def my_fix ( request ): \"\"\"Each returned fixture value will be saved in the global store\"\"\" return \"my_fix # %s \" % request . param # -- the global storage fixture and synthesis creator -- @pytest.fixture ( scope = 'session' , autouse = True ) def store (): # setup: init the store store = OrderedDict () yield store # teardown: here you can collect all print ( dict ( store [ 'my_fix' ])) other teardown hooks If you use another teardown hook, you can still retrieve your 'store' fixture by using the get_fixture_value(request, 'store') utility function provided in this library. yield_fixture In old versions of pytest, you have to use @pytest.yield_fixture to be allowed to use yield in a fixture. 3- Creating \"results bags\" fixtures to collect test artifacts \u00b6 Now we are able to store fixtures. But what about the data that we create during the tests ? It can be accuracy results, etc. For this, simply use create_results_bag_fixture() to create \"results bags\" fixtures where you can put any data you want: from collections import OrderedDict from random import random import pytest from pytest_harvest import create_results_bag_fixture def my_algorithm ( param , data ): # let's return a random accuracy ! return random () @pytest.fixture ( params = [ 'A' , 'B' , 'C' ]) def dataset ( request ): return \"my dataset # %s \" % request . param @pytest.mark.parametrize ( \"algo_param\" , [ 1 , 2 ], ids = str ) def test_my_app_bench ( algo_param , dataset , results_bag ): \"\"\" This test applies the algorithm with various parameters (`algo_param`) on various datasets (`dataset`). Accuracies are stored in a results bag (`results_bag`) \"\"\" # apply the algorithm with param `algo_param` on dataset `dataset` accuracy = my_algorithm ( algo_param , dataset ) # store it in the results bag results_bag . accuracy = accuracy # -- the results bag fixture -- # note: depending on your pytest version, the name used by pytest might be # the variable name (left) or the one you provide in the 'name' argument so # make sure they are identical! results_bag = create_results_bag_fixture ( 'store' , name = \"results_bag\" ) # -- the global storage fixture and synthesis creator -- @pytest.fixture ( scope = 'session' , autouse = True ) def store ( request ): # setup: init the store store = OrderedDict () yield store # teardown: here you can collect all print ( dict ( store [ 'results_bag' ])) We can see the correct results collected: { 'path/to/test_file.py::::test_my_app_bench[A-1]' : ResultsBag : { 'accuracy' : 0.2630766637159053 }, 'path/to/test_file.py::::test_my_app_bench[A-2]' : ResultsBag : { 'accuracy' : 0.6720533462346249 }, 'path/to/test_file.py::::test_my_app_bench[B-1]' : ResultsBag : { 'accuracy' : 0.9121353916881674 }, 'path/to/test_file.py::::test_my_app_bench[B-2]' : ResultsBag : { 'accuracy' : 0.9401074040573346 }, 'path/to/test_file.py::::test_my_app_bench[C-1]' : ResultsBag : { 'accuracy' : 0.01619034700438804 }, 'path/to/test_file.py::::test_my_app_bench[C-2]' : ResultsBag : { 'accuracy' : 0.8027244886806986 } } We can of course combine this with the test status and parameters (we saw above how to collect them) if we want to create a synthesis table. This complete story will be available on pytest-patterns . results bag fixtures' storage You declare the storage used in the arguments of create_results_bag_fixture . As this relies on @saved_fixture , you can use both a variable or a session/module-scoped fixture name as we saw in previous chapter. 4- Creating a Synthesis table \u00b6 Now that we know how to retrieve pytest status and parameters how to store and retrieve fixtures and how to store and retrieve applicative results We can create a synthesis table containing all information available. This is very easy: instead of calling get_session_synthesis_dct with no parameters, give it your store object. Since we want to create a table, we will use the flatten and flatten_more options so that the result does not contain nested dictionaries for the parameters, fixtures, and result bags. Finally we decide that we want the durations expressed in ms (pytest measures them in seconds by default). # retrieve the synthesis, merged with the fixture store results_dct = get_session_synthesis_dct ( session , fixture_store = store , flatten = True , flatten_more = 'results_bag' , durations_in_ms = True ) We can print the first entry: >>> pprint ( dict ( next ( iter ( results_dct . values ())))) { 'pytest_obj' : < function test_my_app_bench at 0x0000000004FF6A60 > , 'pytest_status' : 'passed' , 'pytest_duration_ms' : 0.0 , 'dataset' : 'A' , 'algo_param' : 1 , 'accuracy' : 0.2630766637159053 } We see that all information is available at the same level: pytest status and duration, parameters ( dataset and algo_param ), and results ( accuracy ). Transforming such a flattened dictionary in a table is very easy with pandas : import pandas as pd results_df = pd . DataFrame . from_dict ( results_dct , orient = 'index' ) # (a) remove the full test id path results_df . index = results_df . index . to_series () \\ . apply ( lambda test_id : test_id . split ( '::' )[ - 1 ]) # (b) drop pytest object column results_df . drop ([ 'pytest_obj' ], axis = 1 , inplace = True ) And finally we can use pandas or tabulate to export the result in csv or markdown format: # csv format print ( results_df . to_csv ()) # github markdown format from tabulate import tabulate print ( tabulate ( results_df , headers = 'keys' )) status duration_ms algo_param dataset accuracy test_my_app_bench[A-1] passed 1.00017 1 A 0.313807 test_my_app_bench[A-2] passed 0 2 A 0.0459802 test_my_app_bench[B-1] passed 0 1 B 0.638511 test_my_app_bench[B-2] passed 0 2 B 0.10418 test_my_app_bench[C-1] passed 0 1 C 0.287151 test_my_app_bench[C-2] passed 0 2 C 0.19437 Duration calculation The duration field is directly extracted from pytest . Currently pytest computes durations using the time method, which might not be as accurate as other methods - see opened discussion here . If you need more precise duration benchmarking now , of if you need to measure the duration of a specific sub-function instead of the duration of the whole test function call, use pytest-benchmark . In the long run, the author thinks that pytest will hopefully provide more precise duration estimates, and therefore you will be able to get similar results in the table outputted above (+ possibly a @repeat(n) parameter on top of your test function if you wish to repeat it several times and compare the durations). 5- Partial synthesis (module, function) and synthesis tests \u00b6 We have seen above that you can get the pytest session object from many different teardown hooks. In addition, you can even access it from inside a test! In that case all information will not be available, but if the synthesis test is located after the test function of interest in execution order, it will be ok. To be sure to only get results you're interested in, the special filter argument allows you to only select parts of the test nodes to create the synthesis: # a module-scoped store @pytest.fixture ( scope = 'module' , autouse = True ) def store (): return OrderedDict () # results bag fixture my_results = create_results_bag_fixture ( 'store' , name = 'my_results' ) def test_foo ( my_results ): ... def test_synthesis ( request , store ): # get partial results concerning `test_foo` results_dct = get_session_synthesis_dct ( request . session , filter = test_foo , fixture_store = store ) # you can assert / report using the `results_dct` here See help(get_session_synthesis_dct) for details: for example you can include in this filter a list, and it can contain module names too. Complete example \u00b6 A module-scoped complete example with parameters, fixtures, and results bag can be found in two versions: here with no customization (leveraging the default fixtures) here to perform the exact same behaviour but with custom store and results bag.","title":"Advanced Usage"},{"location":"advanced_usage/#advanced-usage","text":"You have seen in the introduction how to use the pre-wired fixtures to perform most comon tasks. The sections below explore, for each topic, the internals that are used behind the scenes, so that you are able to create custom behaviours if the default ones do not match your needs.","title":"Advanced Usage"},{"location":"advanced_usage/#0-prerequisite-how-to-write-session-teardown-code","text":"In order to be able to retrieve all the information that we will store, we will need to execute our retrieval/synthesis code at the end of the entire test session . pytest currently provides several ways to do this: through a test (put it at the end of the latest test file to ensure execution at the end): def test_synthesis ( request ): # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) through a generator (yield) session-scoped fixture (put this in any of your test files or in the conftest.py file): # Note: for pytest<3.0 you have to use @pytest.yield_fixture instead @pytest.fixture ( scope = 'session' , autouse = True ) def my_cooler_session_finish ( request ): yield # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) through a normal session-scoped fixture (put this in any of your test files or in the conftest.py file): @pytest.fixture ( scope = \"session\" , autouse = True ) def my_session_finish ( request ): def _end (): # you can access the session from the injected 'request': session = request . session print ( \"<Put here your synthesis code>\" ) request . addfinalizer ( _end ) through the session finish hook (you have to write this function in the conftest.py file): def pytest_sessionfinish ( session , exitstatus ): print ( \"<Put here your synthesis code>\" ) All seem completely equivalent for the usage of pytest_harvest . I personally prefer the \"fixture\" style because you can have several of them instead of a monolithic teardown hook. Besides they can be put in the test files so I typically put them as close as possible to the tests that store the data, so as to ensure maintainability (data creation/storage and data retrieval/synthesis code are in the same file).","title":"0- Prerequisite: how to write session teardown code"},{"location":"advanced_usage/#1-collecting-tests-status-and-parameters","text":"Pytest already stores some information out of the box concerning tests that have run. In addition you can follow this example to retrieve more, but it requires you to write a hook so it is not very convenient. Instead, you can easily retrieve all of that thanks to the get_session_synthesis_dct(session) utility function. For example let's assume you have this parametrized test with a parametrized fixture: # unparametrized fixture @pytest.fixture def dummy (): return \"hey there !\" # parametrized fixture @pytest.fixture ( param = [ 1 , 2 ]) def a_number_str ( request ): return \"my_fix # %s \" % request . param # parametrized test using the fixtures @pytest.mark.parametrize ( 'p' , [ 'hello' , 'world' ], ids = str ) def test_foo ( p , a_number_str , dummy ): print ( p + a_number_str ) When running it, 4 tests are executed: >>> pytest ============================= test session starts ============================= collected 4 items path/to/test_file.py::test_foo [ 1 -hello ] PASSED [ 25 % ] path/to/test_file.py::test_foo [ 1 -world ] PASSED [ 50 % ] path/to/test_file.py::test_foo [ 2 -hello ] PASSED [ 75 % ] path/to/test_file.py::test_foo [ 2 -world ] PASSED [ 100 % ] ========================== 4 passed in 0 .11 seconds =========================== let's retrieve the available information at the end of the session. The easiest way is to write a \"last test\" and make sure it is executed after all the other as shown below, but there are other ways as shown above : from pytest_harvest import get_session_synthesis_dct def test_synthesis ( request ): synth_dct = get_session_synthesis_dct ( request . session , status_details = True ) print ( dict ( synth_dct )) It yields { 'path/to/test_file.py::test_foo[1-hello]' : { 'pytest_duration' : 0.0010001659393310547 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 1 , 'p' : 'hello' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0010001659393310547 ), 'setup' : ( 'passed' , 0.013001203536987305 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[1-world]' : { 'pytest_duration' : 0.0 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 1 , 'p' : 'world' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0 ), 'setup' : ( 'passed' , 0.0 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[2-hello]' : { 'pytest_duration' : 0.0010001659393310547 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 2 , 'p' : 'hello' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0010001659393310547 ), 'setup' : ( 'passed' , 0.0010001659393310547 ), 'teardown' : ( 'passed' , 0.0 ) } }, 'path/to/test_file.py::test_foo[2-world]' : { 'pytest_duration' : 0.0 , 'pytest_obj' : < function test_foo at 0x0000000004C13B70 > , 'pytest_params' : { 'a_number_str_param' : 2 , 'p' : 'world' }, 'pytest_status' : 'passed' , 'pytest_status_details' : { 'call' : ( 'passed' , 0.0 ), 'setup' : ( 'passed' , 0.0010001659393310547 ), 'teardown' : ( 'passed' , 0.0 ) } } } As you can see for each test node id you get a dictionary containing 'pytest_obj' the object containing the test code 'pytest_status' the status and 'pytest_duration' the duration of the test 'pytest_params' the parameters used in this test (both in the test function AND the fixture) 'pytest_status_details' a dictionary containing the status details for each pytest internal stages: setup, call, and teardown. status and duration aggregation that the global status corresponds to an aggregation of the status of each of those stages (setup, call, teardown), but the global duration is only the duration of the \"call\" stage - after all we do not care about how long it took to setup and teardown. In addition, you can also use the following companion methods : filter_session_items(session, filter=None) is the filtering method used behind the scenes. pytest_item_matches_filter is the inner method used to test if a single item matches the filter. get_all_pytest_param_names(session) lists all unique parameter names used, with optional filtering capabilities is_pytest_incomplete(item) , get_pytest_status(item) , get_pytest_param_names(item) and get_pytest_params(item) can be used to analyse a specific item in session.items directly without creating the dictionary. Finally let's have a closer look above. It seems that after all we have collected the fixtures, right ? For example we see 'a_number_str_param': 2 . But beware, this is not the fixture. It is the parameter used by the fixture 'a_number_str' . To be convinced look at its type: it is an integer, not a string! A more obvious way to confirm that the fixtures are not available, is to see that the 'dummy' fixture does not appear at all: indeed it had no parameters. To conclude: the get_session_synthesis_dct(session) utility function allows you to collect many information about the tests, but not the fixtures. To do that you have to use the mechanisms below.","title":"1- Collecting tests status and parameters"},{"location":"advanced_usage/#2-storingretrieving-fixtures","text":"You can either choose to store fixtures in a plain old variable, or in another, session-scoped, fixture. Both can be achieved using the same decorator @saved_fixture(store) .","title":"2- Storing/retrieving fixtures"},{"location":"advanced_usage/#a-storing-in-a-variable","text":"Let's create a store. It should be a dict-like object: # Create a global store STORE = dict () In order to store all created fixture values in this object, simply decorate your fixture with @saved_fixture(STORE) : from pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = [ 1 , 2 ]) @saved_fixture ( STORE ) def my_fix ( request ): \"\"\"Each returned fixture value will be saved in the global store\"\"\" return \"my_fix # %s \" % request . param You can then retrieve the available information at the end of the session (put this in your session teardown ): print ( dict ( STORE [ 'my_fix' ])) Each saved fixture appears as an ordered dictionary stored under a global key that has its name (here 'my_fix'). In this dictionary, the keys are the test node ids, and the values are the fixture values: { 'path/to/test_file.py::test_foo[1]' : 'my_fix #1' , 'path/to/test_file.py::test_foo[2]' : 'my_fix #2' } Note: you can change the key used in the global storage with the key= argument of @saved_fixture .","title":"a- Storing in a variable"},{"location":"advanced_usage/#b-storing-in-a-fixture","text":"You might want your store to be a fixture itself, instead of a global variable. It is possible if it is session- or module-scoped (in the same module than where it is used). Simply use its name in @saved_fixture and it will work as expected. This enables you to make the code even more readable because you can put the synthesis code in the teardown part of the storage fixture: from pytest from pytest_harvest import saved_fixture @pytest.fixture ( params = [ 1 , 2 ]) @saved_fixture ( \"store\" ) def my_fix ( request ): \"\"\"Each returned fixture value will be saved in the global store\"\"\" return \"my_fix # %s \" % request . param # -- the global storage fixture and synthesis creator -- @pytest.fixture ( scope = 'session' , autouse = True ) def store (): # setup: init the store store = OrderedDict () yield store # teardown: here you can collect all print ( dict ( store [ 'my_fix' ])) other teardown hooks If you use another teardown hook, you can still retrieve your 'store' fixture by using the get_fixture_value(request, 'store') utility function provided in this library. yield_fixture In old versions of pytest, you have to use @pytest.yield_fixture to be allowed to use yield in a fixture.","title":"b- Storing in a fixture"},{"location":"advanced_usage/#3-creating-results-bags-fixtures-to-collect-test-artifacts","text":"Now we are able to store fixtures. But what about the data that we create during the tests ? It can be accuracy results, etc. For this, simply use create_results_bag_fixture() to create \"results bags\" fixtures where you can put any data you want: from collections import OrderedDict from random import random import pytest from pytest_harvest import create_results_bag_fixture def my_algorithm ( param , data ): # let's return a random accuracy ! return random () @pytest.fixture ( params = [ 'A' , 'B' , 'C' ]) def dataset ( request ): return \"my dataset # %s \" % request . param @pytest.mark.parametrize ( \"algo_param\" , [ 1 , 2 ], ids = str ) def test_my_app_bench ( algo_param , dataset , results_bag ): \"\"\" This test applies the algorithm with various parameters (`algo_param`) on various datasets (`dataset`). Accuracies are stored in a results bag (`results_bag`) \"\"\" # apply the algorithm with param `algo_param` on dataset `dataset` accuracy = my_algorithm ( algo_param , dataset ) # store it in the results bag results_bag . accuracy = accuracy # -- the results bag fixture -- # note: depending on your pytest version, the name used by pytest might be # the variable name (left) or the one you provide in the 'name' argument so # make sure they are identical! results_bag = create_results_bag_fixture ( 'store' , name = \"results_bag\" ) # -- the global storage fixture and synthesis creator -- @pytest.fixture ( scope = 'session' , autouse = True ) def store ( request ): # setup: init the store store = OrderedDict () yield store # teardown: here you can collect all print ( dict ( store [ 'results_bag' ])) We can see the correct results collected: { 'path/to/test_file.py::::test_my_app_bench[A-1]' : ResultsBag : { 'accuracy' : 0.2630766637159053 }, 'path/to/test_file.py::::test_my_app_bench[A-2]' : ResultsBag : { 'accuracy' : 0.6720533462346249 }, 'path/to/test_file.py::::test_my_app_bench[B-1]' : ResultsBag : { 'accuracy' : 0.9121353916881674 }, 'path/to/test_file.py::::test_my_app_bench[B-2]' : ResultsBag : { 'accuracy' : 0.9401074040573346 }, 'path/to/test_file.py::::test_my_app_bench[C-1]' : ResultsBag : { 'accuracy' : 0.01619034700438804 }, 'path/to/test_file.py::::test_my_app_bench[C-2]' : ResultsBag : { 'accuracy' : 0.8027244886806986 } } We can of course combine this with the test status and parameters (we saw above how to collect them) if we want to create a synthesis table. This complete story will be available on pytest-patterns . results bag fixtures' storage You declare the storage used in the arguments of create_results_bag_fixture . As this relies on @saved_fixture , you can use both a variable or a session/module-scoped fixture name as we saw in previous chapter.","title":"3- Creating \"results bags\" fixtures to collect test artifacts"},{"location":"advanced_usage/#4-creating-a-synthesis-table","text":"Now that we know how to retrieve pytest status and parameters how to store and retrieve fixtures and how to store and retrieve applicative results We can create a synthesis table containing all information available. This is very easy: instead of calling get_session_synthesis_dct with no parameters, give it your store object. Since we want to create a table, we will use the flatten and flatten_more options so that the result does not contain nested dictionaries for the parameters, fixtures, and result bags. Finally we decide that we want the durations expressed in ms (pytest measures them in seconds by default). # retrieve the synthesis, merged with the fixture store results_dct = get_session_synthesis_dct ( session , fixture_store = store , flatten = True , flatten_more = 'results_bag' , durations_in_ms = True ) We can print the first entry: >>> pprint ( dict ( next ( iter ( results_dct . values ())))) { 'pytest_obj' : < function test_my_app_bench at 0x0000000004FF6A60 > , 'pytest_status' : 'passed' , 'pytest_duration_ms' : 0.0 , 'dataset' : 'A' , 'algo_param' : 1 , 'accuracy' : 0.2630766637159053 } We see that all information is available at the same level: pytest status and duration, parameters ( dataset and algo_param ), and results ( accuracy ). Transforming such a flattened dictionary in a table is very easy with pandas : import pandas as pd results_df = pd . DataFrame . from_dict ( results_dct , orient = 'index' ) # (a) remove the full test id path results_df . index = results_df . index . to_series () \\ . apply ( lambda test_id : test_id . split ( '::' )[ - 1 ]) # (b) drop pytest object column results_df . drop ([ 'pytest_obj' ], axis = 1 , inplace = True ) And finally we can use pandas or tabulate to export the result in csv or markdown format: # csv format print ( results_df . to_csv ()) # github markdown format from tabulate import tabulate print ( tabulate ( results_df , headers = 'keys' )) status duration_ms algo_param dataset accuracy test_my_app_bench[A-1] passed 1.00017 1 A 0.313807 test_my_app_bench[A-2] passed 0 2 A 0.0459802 test_my_app_bench[B-1] passed 0 1 B 0.638511 test_my_app_bench[B-2] passed 0 2 B 0.10418 test_my_app_bench[C-1] passed 0 1 C 0.287151 test_my_app_bench[C-2] passed 0 2 C 0.19437 Duration calculation The duration field is directly extracted from pytest . Currently pytest computes durations using the time method, which might not be as accurate as other methods - see opened discussion here . If you need more precise duration benchmarking now , of if you need to measure the duration of a specific sub-function instead of the duration of the whole test function call, use pytest-benchmark . In the long run, the author thinks that pytest will hopefully provide more precise duration estimates, and therefore you will be able to get similar results in the table outputted above (+ possibly a @repeat(n) parameter on top of your test function if you wish to repeat it several times and compare the durations).","title":"4- Creating a Synthesis table"},{"location":"advanced_usage/#5-partial-synthesis-module-function-and-synthesis-tests","text":"We have seen above that you can get the pytest session object from many different teardown hooks. In addition, you can even access it from inside a test! In that case all information will not be available, but if the synthesis test is located after the test function of interest in execution order, it will be ok. To be sure to only get results you're interested in, the special filter argument allows you to only select parts of the test nodes to create the synthesis: # a module-scoped store @pytest.fixture ( scope = 'module' , autouse = True ) def store (): return OrderedDict () # results bag fixture my_results = create_results_bag_fixture ( 'store' , name = 'my_results' ) def test_foo ( my_results ): ... def test_synthesis ( request , store ): # get partial results concerning `test_foo` results_dct = get_session_synthesis_dct ( request . session , filter = test_foo , fixture_store = store ) # you can assert / report using the `results_dct` here See help(get_session_synthesis_dct) for details: for example you can include in this filter a list, and it can contain module names too.","title":"5- Partial synthesis (module, function) and synthesis tests"},{"location":"advanced_usage/#complete-example","text":"A module-scoped complete example with parameters, fixtures, and results bag can be found in two versions: here with no customization (leveraging the default fixtures) here to perform the exact same behaviour but with custom store and results bag.","title":"Complete example"},{"location":"api_reference/","text":"API reference \u00b6 1. Available fixtures \u00b6 The following fixtures can be used in your tests as soon as the library is installed (no explicit plugin activation is required). results_bag \u00b6 A \"results bag\" fixture: a dictionary where you can store anything (results, context, etc.) during your tests execution. It offers a \"much\"-like api: you can access all entries using the object protocol such as in results_bag.a = 1 . This fixture has function-scope so a new, empty instance is injected in each test node. There are several ways to gather all results after they have been stored. To get the raw stored results, use the fixture_store fixture: fixture_store['results_bag'] will contain all result bags for all tests. If you are interested in both the stored results AND some stored fixture values (through @saved_fixture ), you might rather wish to leverage the following helpers: use one of the session_results_dct , module_results_dct , session_results_df or module_results_df fixtures. They contain all available information, in a nicely summarized way. use the get_session_synthesis_dct(session) helper method to create a similar synthesis than the above with more customization capabilities. If you wish to create custom results bags similar to this one (for example to create several with different names), use create_results_bag_fixture . See basic and advanced documentation for details. fixture_store \u00b6 A 'fixture store' fixture: a dictionary where fixture instances can be saved. By default all fixtures decorated with @saved_fixture are saved in this store. the results_bag fixture is also saved in this store. To retrieve the contents of the store, you can: create a test using this fixture and make sure that it is executed after all others. access this fixture from a dependent fixture and read its value in the setup or teardown script. access this fixture from the request fixture using the get_fixture_value helper method. This fixture has session scope so it is unique across the whole session. session_results_dct \u00b6 This fixture contains a synthesis dictionary for all tests completed \"so far\", with 'full' id format. It includes contents from the default fixture_store , including results_bag . Behind the scenes it relies on get_session_synthesis_dct . This fixture has a function scope because we want its contents to be refreshed every time it is needed. See documentation for details. module_results_dct \u00b6 Same than session_results_dct but with module scope. session_results_df \u00b6 This fixture contains a synthesis dataframe for all tests completed \"so far\" in the module of the caller, with 'function' id format. It includes contents from the default fixture_store , including results_bag . It is basically just a transformation of the session_results_dct fixture into a pandas DataFrame . If pytest-steps is installed, the step ids will be extracted and the dataframe index will be multi-level (test id without step, step id). This fixture has a function scope because we want its contents to be refreshed every time it is needed. See documentation for details. module_results_df \u00b6 Same than session_results_df but with module scope. 2. Additional symbols \u00b6 a- Basic \u00b6 @saved_fixture \u00b6 @saved_fixture ( store = 'fixture_store' , # type: Union[str, Dict[str, Any]] key = None , # type: str views = None , # type: Dict[str, Callable[[Any], Any]] save_raw = None , # type: bool ) Decorates a fixture so that it is saved in store . store can be a dict-like variable or a string representing a fixture name to a dict-like session-scoped fixture. By default it uses the global 'fixture_store' fixture provided by this plugin. After executing all tests, <store> will contain a new item under key <key> (default is the name of the fixture). This item is a dictionary : for each test node where the fixture was setup. import pytest from pytest_harvest import saved_fixture @pytest.fixture @saved_fixture def dummy (): return 1 def test_dummy ( dummy ): pass def test_synthesis ( fixture_store ): print ( fixture_store [ 'dummy' ]) Note that for session-scoped and module-scoped fixtures, not all test ids will appear in the store - only those for which the fixture was (re)created. Users can save additional views created from the fixture instance by applying transforms (callable functions). To do this, users can provide a dictionary under the views argument, containing a {<key>: <procedure>} dict-like. For each entry, <procedure> will be applied on the fixture instance, and the result will be stored under <key> . save_raw controls whether the fixture instance should still be saved in this case (default: True if views is None , False otherwise). Parameters store : a dict-like object or a fixture name corresponding to a dict-like object. in this dictionary, a new entry will be added for the fixture. This entry will contain a dictionary : for each test node. By default fixtures are stored in the `fixture_store``fixture. key : the name associated with the stored fixture in the store. By default this is the fixture name. views : an optional dictionary that can be provided to store views created from the fixture, rather than (or in addition to, if save_raw=True ) the fixture itself. The dict should contain a {<key>: <procedure>} dict-like. For each entry, <procedure> will be applied on the fixture instance, and the result will be stored under <key> . save_raw : controls whether the fixture instance should be saved. None (Default) is an automatic behaviour meaning \" True if views is None , False otherwise\". Returns : a fixture that will be stored See basic and advanced documentation for details. ResultsBag class \u00b6 The default type for result bags, used in the results_bag fixture and create_results_bag_fixture method. It is a simple 'Munch', that is, a dual object/dict. It is hashable with a not very interesting hash, but at least a unique one in a python session (id(self)). b- Intermediate \u00b6 create_results_bag_fixture(...) \u00b6 create_results_bag_fixture ( store , # type: Union[str, Dict[str, Any]] name = 'results_bag' , # type: str bag_type = None , # type: Type[Any] ) Creates a \"results bag\" fixture with name name stored in the given store (under key= name ). By default results bags are instances of ResultsBag but you can provide another bag_type if needed. Parameters store: a dict-like object or a fixture name corresponding to a dict-like object. in this dictionary, a new entry will be added for the fixture. This entry will contain a dictionary : for each test node. name: the name associated with the stored fixture in the global store. By default this is 'results_bag'. bag_type: the type of object to create as a results bag. Default: ResultsBag c- Advanced \u00b6 get_session_synthesis_dct(...) \u00b6 get_session_synthesis_dct ( session_or_request , test_id_format = 'full' , # status_details = False , # type: bool durations_in_ms = False , # type: bool pytest_prefix = None , # type: bool filter = None , # type: Any filter_incomplete = True , # type: bool flatten = False , # type: bool fixture_store = None , # type: Union[Mapping[str, Any], Iterable[Mapping[str, Any]]] flatten_more = None # type: Union[str, Iterable[str], Mapping[str, str]] ) Returns a dictionary containing a synthesis of what is available currently in the provided pytest session object. For each entry, the key is the test id, and the value is a dictionary containing: 'pytest_obj' : the object under test, typically a test function 'pytest_status' : the overall status ( 'failing' , 'skipped' , 'passed' ) 'pytest_duration' : the duration of the 'call' step. By default this is the pytest unit (s) but if you set durations_in_ms=True it becomes (ms) 'pytest_status_details' : a dictionary containing step-by-step status details for all pytest steps ( 'setup' , 'call' , 'teardown' ). This is only included if status_details=True (not by default) It is possible to process the test id (the keys) using the test_id_format option. Let's assume that the id is pytest_steps/tests_raw/test_wrapped_in_class.py::TestX::()::test_easy[p1-p2] . Here are the returned test ids depending on the selected test_id_format 'function' will return test_easy[p1-p2] 'class' will return TestX::()::test_easy[p1-p2] 'module' will return test_wrapped_in_class.py::TestX::()::test_easy[...] 'full' will return the original id (this is the default behaviour) In addition one can provide a custom string handling function that will be called for each test id to process. The 'pytest' prefix in front of all these items (except pytest_obj ) is by default added in non-flatten mode and removed in flatten mode. To force one of these you can set pytest_prefix to True or False . An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. An optional collection of storage objects can be provided, so as to merge them into the resulting dictionary. Finally a flatten_output option allows users to get a flat dictionary output instead of nested status details, parameters dict, and storage dicts. Parameters : session : a pytest session object. test_id_format : one of 'function', 'class', 'module', or 'full' (default), or a custom test id processing function. status_details : a flag indicating if pytest status details per stage (setup/call/teardown) should be included. Default= False : only the pytest status summary is provided. durations_in_ms : by default pytest measures durations in seconds so they are outputed in this unit. You can turn the flag to True to output milliseconds instead. pytest_prefix : to add (True) or remove (False) the 'pytest_' prefix in front of status, duration and status details. Typically useful in flatten mode when the names are not ambiguous. By default it is None, which means =(not flatten) filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use module names. See pytest_item_matches_filter . filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True, default). flatten : a boolean (default False ) indicating if the resulting dictionary should be flattened. If it set to True , the 3 nested dictionaries (pytest status details, parameters, and optionally storages) will have their contents directly copied in the first level (with a prefix added in case of pytest status details). fixture_store : a singleton or iterable containing dict-like fixture storage objects (see @saved_fixture and create_results_bag_fixture ). If flatten= False the contents of these dictionaries will be added to the output in a dedicated 'fixtures' entry. If flatten=True all of their contents will be included directly. flatten_more : a singleton, iterable or dictionary containing fixture names to flatten one level more in case flatten=True. If a dictionary is provided, the key should be the fixture name, and the value should be a prefix used for flattening its contents Returns : a dictionary where the keys are pytest node ids. Each value is also a dictionary, containing information available from pytest concerning the test node, and optionally storage contents if storage_dcts is provided. filter_session_items(...) \u00b6 filter_session_items ( session , filter = None , # type: Any ) Filters pytest session item in the provided session . An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. Used in get_session_synthesis_dct . Parameters session : a pytest session filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use module names. Returns : an iterable containing possibly filtered session items pytest_item_matches_filter(...) \u00b6 pytest_item_matches_filter ( item , filter ) Returns True if pytest session item item matches filter filter , False otherwise. d- Pytest utils \u00b6 get_all_pytest_fixture_names(...) \u00b6 get_all_pytest_fixture_names ( session , filter = None , # type: Any filter_incomplete = False , # type: bool ) Returns the list of all unique fixture names used in all items in the provided session, with given filter. An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. Parameters session : a pytest session object. filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use modules. filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True). Note: by default incomplete nodes DO APPEAR (this is different from get_session_synthesis_dct behaviour) Returns : a list of fixture names corresponding to the desired filters get_all_pytest_param_names(...) \u00b6 get_all_pytest_param_names ( session , filter = None , # type: Any filter_incomplete = False , # type: bool ) Returns the list of all unique parameter names used in all items in the provided session, with given filter. An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. Parameters session : a pytest session object. filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use modules. filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True). Note: by default incomplete nodes DO APPEAR (this is different from get_session_synthesis_dct behaviour) Returns : a list of parameter names corresponding to the desired filters get_fixture_value(...) \u00b6 get_fixture_value(request, fixture_name) Returns the value associated with fixture named fixture_name , in provided request context. This is just an easy way to use getfixturevalue or getfuncargvalue according to whichever is available in current pytest version. get_pytest_status(...) \u00b6 get_pytest_status(item, durations_in_ms=False, current_request=None) Returns a dictionary containing item's pytest status (success/skipped/failed, duration converted to ms) for each pytest phase, and a tuple synthesizing the information. The synthesis status contains the worst status of all phases (setup/call/teardown), or 'pending' if there are less than 3 phases. The synthesis duration is equal to the duration of the 'call' phase (not to the sum of all phases: indeed, we are mostly interested in the test call itself). Parameters item : a pytest session.item durations_in_ms : by default pytest measures durations in seconds so they are outputed in this unit. You can turn the flag to True to output milliseconds instead. current_request : if a non-None request is provided and the item is precisely the one from the request, then the status will be 'pending' Returns : a tuple ((test_status, test_duration), status_dct) is_pytest_incomplete(...) \u00b6 is_pytest_incomplete(item) Returns True if a pytest item is incomplete - in other words if at least one of the 3 steps (setup/call/teardown) is missing from the available pytest report attached to this item. get_pytest_param_names(...) \u00b6 get_pytest_param_names(item) Returns a list containing a pytest session item's parameters. get_pytest_params(...) \u00b6 Returns a dictionary containing a pytest session item's parameters.","title":"API reference"},{"location":"api_reference/#api-reference","text":"","title":"API reference"},{"location":"api_reference/#1-available-fixtures","text":"The following fixtures can be used in your tests as soon as the library is installed (no explicit plugin activation is required).","title":"1. Available fixtures"},{"location":"api_reference/#results_bag","text":"A \"results bag\" fixture: a dictionary where you can store anything (results, context, etc.) during your tests execution. It offers a \"much\"-like api: you can access all entries using the object protocol such as in results_bag.a = 1 . This fixture has function-scope so a new, empty instance is injected in each test node. There are several ways to gather all results after they have been stored. To get the raw stored results, use the fixture_store fixture: fixture_store['results_bag'] will contain all result bags for all tests. If you are interested in both the stored results AND some stored fixture values (through @saved_fixture ), you might rather wish to leverage the following helpers: use one of the session_results_dct , module_results_dct , session_results_df or module_results_df fixtures. They contain all available information, in a nicely summarized way. use the get_session_synthesis_dct(session) helper method to create a similar synthesis than the above with more customization capabilities. If you wish to create custom results bags similar to this one (for example to create several with different names), use create_results_bag_fixture . See basic and advanced documentation for details.","title":"results_bag"},{"location":"api_reference/#fixture_store","text":"A 'fixture store' fixture: a dictionary where fixture instances can be saved. By default all fixtures decorated with @saved_fixture are saved in this store. the results_bag fixture is also saved in this store. To retrieve the contents of the store, you can: create a test using this fixture and make sure that it is executed after all others. access this fixture from a dependent fixture and read its value in the setup or teardown script. access this fixture from the request fixture using the get_fixture_value helper method. This fixture has session scope so it is unique across the whole session.","title":"fixture_store"},{"location":"api_reference/#session_results_dct","text":"This fixture contains a synthesis dictionary for all tests completed \"so far\", with 'full' id format. It includes contents from the default fixture_store , including results_bag . Behind the scenes it relies on get_session_synthesis_dct . This fixture has a function scope because we want its contents to be refreshed every time it is needed. See documentation for details.","title":"session_results_dct"},{"location":"api_reference/#module_results_dct","text":"Same than session_results_dct but with module scope.","title":"module_results_dct"},{"location":"api_reference/#session_results_df","text":"This fixture contains a synthesis dataframe for all tests completed \"so far\" in the module of the caller, with 'function' id format. It includes contents from the default fixture_store , including results_bag . It is basically just a transformation of the session_results_dct fixture into a pandas DataFrame . If pytest-steps is installed, the step ids will be extracted and the dataframe index will be multi-level (test id without step, step id). This fixture has a function scope because we want its contents to be refreshed every time it is needed. See documentation for details.","title":"session_results_df"},{"location":"api_reference/#module_results_df","text":"Same than session_results_df but with module scope.","title":"module_results_df"},{"location":"api_reference/#2-additional-symbols","text":"","title":"2. Additional symbols"},{"location":"api_reference/#a-basic","text":"","title":"a- Basic"},{"location":"api_reference/#saved_fixture","text":"@saved_fixture ( store = 'fixture_store' , # type: Union[str, Dict[str, Any]] key = None , # type: str views = None , # type: Dict[str, Callable[[Any], Any]] save_raw = None , # type: bool ) Decorates a fixture so that it is saved in store . store can be a dict-like variable or a string representing a fixture name to a dict-like session-scoped fixture. By default it uses the global 'fixture_store' fixture provided by this plugin. After executing all tests, <store> will contain a new item under key <key> (default is the name of the fixture). This item is a dictionary : for each test node where the fixture was setup. import pytest from pytest_harvest import saved_fixture @pytest.fixture @saved_fixture def dummy (): return 1 def test_dummy ( dummy ): pass def test_synthesis ( fixture_store ): print ( fixture_store [ 'dummy' ]) Note that for session-scoped and module-scoped fixtures, not all test ids will appear in the store - only those for which the fixture was (re)created. Users can save additional views created from the fixture instance by applying transforms (callable functions). To do this, users can provide a dictionary under the views argument, containing a {<key>: <procedure>} dict-like. For each entry, <procedure> will be applied on the fixture instance, and the result will be stored under <key> . save_raw controls whether the fixture instance should still be saved in this case (default: True if views is None , False otherwise). Parameters store : a dict-like object or a fixture name corresponding to a dict-like object. in this dictionary, a new entry will be added for the fixture. This entry will contain a dictionary : for each test node. By default fixtures are stored in the `fixture_store``fixture. key : the name associated with the stored fixture in the store. By default this is the fixture name. views : an optional dictionary that can be provided to store views created from the fixture, rather than (or in addition to, if save_raw=True ) the fixture itself. The dict should contain a {<key>: <procedure>} dict-like. For each entry, <procedure> will be applied on the fixture instance, and the result will be stored under <key> . save_raw : controls whether the fixture instance should be saved. None (Default) is an automatic behaviour meaning \" True if views is None , False otherwise\". Returns : a fixture that will be stored See basic and advanced documentation for details.","title":"@saved_fixture"},{"location":"api_reference/#resultsbag-class","text":"The default type for result bags, used in the results_bag fixture and create_results_bag_fixture method. It is a simple 'Munch', that is, a dual object/dict. It is hashable with a not very interesting hash, but at least a unique one in a python session (id(self)).","title":"ResultsBag class"},{"location":"api_reference/#b-intermediate","text":"","title":"b- Intermediate"},{"location":"api_reference/#create_results_bag_fixture","text":"create_results_bag_fixture ( store , # type: Union[str, Dict[str, Any]] name = 'results_bag' , # type: str bag_type = None , # type: Type[Any] ) Creates a \"results bag\" fixture with name name stored in the given store (under key= name ). By default results bags are instances of ResultsBag but you can provide another bag_type if needed. Parameters store: a dict-like object or a fixture name corresponding to a dict-like object. in this dictionary, a new entry will be added for the fixture. This entry will contain a dictionary : for each test node. name: the name associated with the stored fixture in the global store. By default this is 'results_bag'. bag_type: the type of object to create as a results bag. Default: ResultsBag","title":"create_results_bag_fixture(...)"},{"location":"api_reference/#c-advanced","text":"","title":"c- Advanced"},{"location":"api_reference/#get_session_synthesis_dct","text":"get_session_synthesis_dct ( session_or_request , test_id_format = 'full' , # status_details = False , # type: bool durations_in_ms = False , # type: bool pytest_prefix = None , # type: bool filter = None , # type: Any filter_incomplete = True , # type: bool flatten = False , # type: bool fixture_store = None , # type: Union[Mapping[str, Any], Iterable[Mapping[str, Any]]] flatten_more = None # type: Union[str, Iterable[str], Mapping[str, str]] ) Returns a dictionary containing a synthesis of what is available currently in the provided pytest session object. For each entry, the key is the test id, and the value is a dictionary containing: 'pytest_obj' : the object under test, typically a test function 'pytest_status' : the overall status ( 'failing' , 'skipped' , 'passed' ) 'pytest_duration' : the duration of the 'call' step. By default this is the pytest unit (s) but if you set durations_in_ms=True it becomes (ms) 'pytest_status_details' : a dictionary containing step-by-step status details for all pytest steps ( 'setup' , 'call' , 'teardown' ). This is only included if status_details=True (not by default) It is possible to process the test id (the keys) using the test_id_format option. Let's assume that the id is pytest_steps/tests_raw/test_wrapped_in_class.py::TestX::()::test_easy[p1-p2] . Here are the returned test ids depending on the selected test_id_format 'function' will return test_easy[p1-p2] 'class' will return TestX::()::test_easy[p1-p2] 'module' will return test_wrapped_in_class.py::TestX::()::test_easy[...] 'full' will return the original id (this is the default behaviour) In addition one can provide a custom string handling function that will be called for each test id to process. The 'pytest' prefix in front of all these items (except pytest_obj ) is by default added in non-flatten mode and removed in flatten mode. To force one of these you can set pytest_prefix to True or False . An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. An optional collection of storage objects can be provided, so as to merge them into the resulting dictionary. Finally a flatten_output option allows users to get a flat dictionary output instead of nested status details, parameters dict, and storage dicts. Parameters : session : a pytest session object. test_id_format : one of 'function', 'class', 'module', or 'full' (default), or a custom test id processing function. status_details : a flag indicating if pytest status details per stage (setup/call/teardown) should be included. Default= False : only the pytest status summary is provided. durations_in_ms : by default pytest measures durations in seconds so they are outputed in this unit. You can turn the flag to True to output milliseconds instead. pytest_prefix : to add (True) or remove (False) the 'pytest_' prefix in front of status, duration and status details. Typically useful in flatten mode when the names are not ambiguous. By default it is None, which means =(not flatten) filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use module names. See pytest_item_matches_filter . filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True, default). flatten : a boolean (default False ) indicating if the resulting dictionary should be flattened. If it set to True , the 3 nested dictionaries (pytest status details, parameters, and optionally storages) will have their contents directly copied in the first level (with a prefix added in case of pytest status details). fixture_store : a singleton or iterable containing dict-like fixture storage objects (see @saved_fixture and create_results_bag_fixture ). If flatten= False the contents of these dictionaries will be added to the output in a dedicated 'fixtures' entry. If flatten=True all of their contents will be included directly. flatten_more : a singleton, iterable or dictionary containing fixture names to flatten one level more in case flatten=True. If a dictionary is provided, the key should be the fixture name, and the value should be a prefix used for flattening its contents Returns : a dictionary where the keys are pytest node ids. Each value is also a dictionary, containing information available from pytest concerning the test node, and optionally storage contents if storage_dcts is provided.","title":"get_session_synthesis_dct(...)"},{"location":"api_reference/#filter_session_items","text":"filter_session_items ( session , filter = None , # type: Any ) Filters pytest session item in the provided session . An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. Used in get_session_synthesis_dct . Parameters session : a pytest session filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use module names. Returns : an iterable containing possibly filtered session items","title":"filter_session_items(...)"},{"location":"api_reference/#pytest_item_matches_filter","text":"pytest_item_matches_filter ( item , filter ) Returns True if pytest session item item matches filter filter , False otherwise.","title":"pytest_item_matches_filter(...)"},{"location":"api_reference/#d-pytest-utils","text":"","title":"d- Pytest utils"},{"location":"api_reference/#get_all_pytest_fixture_names","text":"get_all_pytest_fixture_names ( session , filter = None , # type: Any filter_incomplete = False , # type: bool ) Returns the list of all unique fixture names used in all items in the provided session, with given filter. An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. Parameters session : a pytest session object. filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use modules. filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True). Note: by default incomplete nodes DO APPEAR (this is different from get_session_synthesis_dct behaviour) Returns : a list of fixture names corresponding to the desired filters","title":"get_all_pytest_fixture_names(...)"},{"location":"api_reference/#get_all_pytest_param_names","text":"get_all_pytest_param_names ( session , filter = None , # type: Any filter_incomplete = False , # type: bool ) Returns the list of all unique parameter names used in all items in the provided session, with given filter. An optional filter can be provided, that can be a singleton or iterable of pytest objects (typically test functions) and/or module names. If this method is called before the end of the pytest session, some nodes might be incomplete, i.e. they will not have data for the three stages (setup/call/teardown). By default these nodes are filtered out but you can set filter_incomplete=False to make them appear. They will have a special 'pending' synthesis status. Parameters session : a pytest session object. filter : a singleton or iterable of pytest objects on which to filter the returned dict on (the returned items will only by pytest nodes for which the pytest object is one of the ones provided). One can also use modules. filter_incomplete : a boolean indicating if incomplete nodes (without the three stages setup/call/teardown) should appear in the results (False) or not (True). Note: by default incomplete nodes DO APPEAR (this is different from get_session_synthesis_dct behaviour) Returns : a list of parameter names corresponding to the desired filters","title":"get_all_pytest_param_names(...)"},{"location":"api_reference/#get_fixture_value","text":"get_fixture_value(request, fixture_name) Returns the value associated with fixture named fixture_name , in provided request context. This is just an easy way to use getfixturevalue or getfuncargvalue according to whichever is available in current pytest version.","title":"get_fixture_value(...)"},{"location":"api_reference/#get_pytest_status","text":"get_pytest_status(item, durations_in_ms=False, current_request=None) Returns a dictionary containing item's pytest status (success/skipped/failed, duration converted to ms) for each pytest phase, and a tuple synthesizing the information. The synthesis status contains the worst status of all phases (setup/call/teardown), or 'pending' if there are less than 3 phases. The synthesis duration is equal to the duration of the 'call' phase (not to the sum of all phases: indeed, we are mostly interested in the test call itself). Parameters item : a pytest session.item durations_in_ms : by default pytest measures durations in seconds so they are outputed in this unit. You can turn the flag to True to output milliseconds instead. current_request : if a non-None request is provided and the item is precisely the one from the request, then the status will be 'pending' Returns : a tuple ((test_status, test_duration), status_dct)","title":"get_pytest_status(...)"},{"location":"api_reference/#is_pytest_incomplete","text":"is_pytest_incomplete(item) Returns True if a pytest item is incomplete - in other words if at least one of the 3 steps (setup/call/teardown) is missing from the available pytest report attached to this item.","title":"is_pytest_incomplete(...)"},{"location":"api_reference/#get_pytest_param_names","text":"get_pytest_param_names(item) Returns a list containing a pytest session item's parameters.","title":"get_pytest_param_names(...)"},{"location":"api_reference/#get_pytest_params","text":"Returns a dictionary containing a pytest session item's parameters.","title":"get_pytest_params(...)"},{"location":"changelog/","text":"Changelog \u00b6 1.7.2 - added __version__ attribute \u00b6 Added __version__ attribute at package level. 1.7.1 - added six dependency \u00b6 It was missing from setup.py . 1.7.0 - @saved_fixture supports all scopes \u00b6 Session-scoped and Module-scoped fixtures are now supported by @saved_fixture . Fixes #17 . Documentation: new API reference page. 1.6.1 - Minor improvements \u00b6 Renamed argument in create_results_bag_fixture to align with the name used in saved_fixture ( store instead of storage ) Now using decopatch for decorator creation. make_saved_fixture can thus be removed, and saved_fixture simplified. Now using latest makefun>=1.5 so that saved_fixture create proper fixture wrappers, using @makefun.wraps . 1.6.0 - improved @saved_fixture + minor dependency change \u00b6 Users can now use @saved_fixture to save not only the fixture, but also some views created from it. This is interesting if each fixture is huge but users just want to save small aspects of it (name, size, etc.). Fixed #21 . Dependency to decorator has been dropped and replaced with makefun . 1.5.0 - Bug fixes concerning fixtures \u00b6 The fixture_store fixture, provided by the plugin, does not have autouse=True anymore. Fixed #20 . get_all_pytest_fixture_names now returns fixtures that are indirectly parametrized, as well as fixtures that are not parametrized. Fixed #19 . 1.4.3 - Better exceptions for @saved_fixture \u00b6 Now raising a better exception if @saved_fixture is used on session- or module-scope fixtures. Fixes #18 1.4.2 - Fixed results bags in presence of steps (2) \u00b6 Another import error was causing results bag to be saved incorrectly in presence of steps. 1.4.1 - Fixed results bags in presence of steps \u00b6 Results bags are now compliant with pytest-steps : there are now one per step. This fixed #16 . 1.4.0 - Removed integration with pytest_steps in default fixtures \u00b6 Integrating pytest-steps in default fixtures seemed like a bad idea because it led to automatic behaviour that could silently raise warnings. Let pytest-steps handle it on its side. 1.3.0 - Better integration with pytest_steps in default fixtures \u00b6 Default fixtures module_results_df and session_results_df now automatically become multi-level indexed when pytest steps is installed and there are steps in the tests. 1.2.1 - Minor: new low-level API \u00b6 New method get_all_pytest_fixture_names to list all fixture names used by items in a session. 1.2.0 - Added column in default dataframe synthesis fixtures \u00b6 Fixtures module_results_df and session_results_df now contains the 'pytest_obj' column. 1.1.0 - New default fixtures + fixture parameter names fix \u00b6 Created 6 fixtures registered by default by the plugin. Fixed #14 : fixture_store in an OrderedDict that can be used as a fixture store, typically in @saved_fixture . results_bag is a ResultsBag -typed results bag. session_results_dct and module_results_dct return a synthesis dictionary for all tests completed \"so far\", respectively in the session or module. They include contents from the default fixture_store , including results_bag . session_results_df and module_results_df are the dataframe equivalents of session_results_dct and module_results_dct The documentation has been updated so that users can get started more quickly by leveraging them. In addition: get_session_synthesis_dct can now take both a session or a request input. If a request is provided, the status of current item will be marked as 'pending', while not started items will be marked as 'unknown'. fixed bug in get_session_synthesis_dct : fixture parameters and saved fixtures where overriding each other in the final dict in flatten=True mode. Now fixture parameters appear as '<fixture_name>_param' . Fixed #15 . @saved_fixture can now be used without arguments. By default it will store fixtures in the default session-scoped 'fixture_store' fixture. HARVEST_PREFIX moved to common.py and is now exported at package level. 1.0.1 - Ordering bug fix \u00b6 Fixed pytest ordering issue, by relying on place_as . See #18 1.0.0 - new methods for pytest session analysis \u00b6 New methods are provided to analyse pytest session results: - filter_session_items(session, filter=None) is the filtering method used behind several functions in this package - it can be used independently. pytest_item_matches_filter is the inner method used to test if a single item matches the filter. - get_all_pytest_param_names(session, filter=None, filter_incomplete=False) lists all unique parameter names used in pytest session items, with optional filtering capabilities. Fixes #12 - is_pytest_incomplete(item) , get_pytest_status(item) , get_pytest_param_names(item) and get_pytest_params(item) allow users to analyse a specific item. 0.9.0 - get_session_synthesis_dct : filter bugfix + test id formatter \u00b6 get_session_synthesis_dct : filter now correctly handles class methods. Fixed #11 new test_id_format option to process test ids. Fixed #9 0.8.0 - Documentation + better filters in get_session_synthesis_dct \u00b6 Documentation: added a section about creating the synthesis table from inside a test function (fixes #4 ). Also, added a link to a complete example file. get_session_synthesis_dct : filter argument can now contain module names (fixed #7 ). Also now the function filters out incomplete tests by default. A new filter_incomplete argument can be used to display them again (fixed #8 ). 0.7.0 - Documentation + get_session_synthesis_dct improvements 2 \u00b6 Results bags do not measure execution time anymore since this is much less accurate than pytest duration. Fixes #6 get_session_synthesis_dct does not output the stage by stage details (setup/call/teardown) anymore by default, but a new option status_details allows users to enable them. Fixes #5 get_session_synthesis_dct has also 2 new options durations_in_ms and pytest_prefix to better control the output. Improved documentation. 0.6.0 - get_session_synthesis_dct improvements \u00b6 get_session_synthesis_dct now has a test object filter , a flatten option, and and can now take optional storage objects as input to create a fully merged dictionary. See help(get_session_synthesis_dct) for details. Fixes #3 . 0.5.1 - Fixed bug with pytest 2.x \u00b6 Fixed #2 . 0.5.0 - First public version \u00b6 First version validated against the data science benchmark pattern (yet to be published) get_session_synthesis_dct method to collect various test information already available @saved_fixture decorator supports both a variable or a fixture for the storage create_results_bag_fixture to create results bags Documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#172-added-__version__-attribute","text":"Added __version__ attribute at package level.","title":"1.7.2 - added __version__ attribute"},{"location":"changelog/#171-added-six-dependency","text":"It was missing from setup.py .","title":"1.7.1 - added six dependency"},{"location":"changelog/#170-saved_fixture-supports-all-scopes","text":"Session-scoped and Module-scoped fixtures are now supported by @saved_fixture . Fixes #17 . Documentation: new API reference page.","title":"1.7.0 - @saved_fixture supports all scopes"},{"location":"changelog/#161-minor-improvements","text":"Renamed argument in create_results_bag_fixture to align with the name used in saved_fixture ( store instead of storage ) Now using decopatch for decorator creation. make_saved_fixture can thus be removed, and saved_fixture simplified. Now using latest makefun>=1.5 so that saved_fixture create proper fixture wrappers, using @makefun.wraps .","title":"1.6.1 - Minor improvements"},{"location":"changelog/#160-improved-saved_fixture-minor-dependency-change","text":"Users can now use @saved_fixture to save not only the fixture, but also some views created from it. This is interesting if each fixture is huge but users just want to save small aspects of it (name, size, etc.). Fixed #21 . Dependency to decorator has been dropped and replaced with makefun .","title":"1.6.0 - improved @saved_fixture + minor dependency change"},{"location":"changelog/#150-bug-fixes-concerning-fixtures","text":"The fixture_store fixture, provided by the plugin, does not have autouse=True anymore. Fixed #20 . get_all_pytest_fixture_names now returns fixtures that are indirectly parametrized, as well as fixtures that are not parametrized. Fixed #19 .","title":"1.5.0 - Bug fixes concerning fixtures"},{"location":"changelog/#143-better-exceptions-for-saved_fixture","text":"Now raising a better exception if @saved_fixture is used on session- or module-scope fixtures. Fixes #18","title":"1.4.3 - Better exceptions for @saved_fixture"},{"location":"changelog/#142-fixed-results-bags-in-presence-of-steps-2","text":"Another import error was causing results bag to be saved incorrectly in presence of steps.","title":"1.4.2 - Fixed results bags in presence of steps (2)"},{"location":"changelog/#141-fixed-results-bags-in-presence-of-steps","text":"Results bags are now compliant with pytest-steps : there are now one per step. This fixed #16 .","title":"1.4.1 - Fixed results bags in presence of steps"},{"location":"changelog/#140-removed-integration-with-pytest_steps-in-default-fixtures","text":"Integrating pytest-steps in default fixtures seemed like a bad idea because it led to automatic behaviour that could silently raise warnings. Let pytest-steps handle it on its side.","title":"1.4.0 - Removed integration with pytest_steps in default fixtures"},{"location":"changelog/#130-better-integration-with-pytest_steps-in-default-fixtures","text":"Default fixtures module_results_df and session_results_df now automatically become multi-level indexed when pytest steps is installed and there are steps in the tests.","title":"1.3.0 - Better integration with pytest_steps in default fixtures"},{"location":"changelog/#121-minor-new-low-level-api","text":"New method get_all_pytest_fixture_names to list all fixture names used by items in a session.","title":"1.2.1 - Minor: new low-level API"},{"location":"changelog/#120-added-column-in-default-dataframe-synthesis-fixtures","text":"Fixtures module_results_df and session_results_df now contains the 'pytest_obj' column.","title":"1.2.0 - Added column in default dataframe synthesis fixtures"},{"location":"changelog/#110-new-default-fixtures-fixture-parameter-names-fix","text":"Created 6 fixtures registered by default by the plugin. Fixed #14 : fixture_store in an OrderedDict that can be used as a fixture store, typically in @saved_fixture . results_bag is a ResultsBag -typed results bag. session_results_dct and module_results_dct return a synthesis dictionary for all tests completed \"so far\", respectively in the session or module. They include contents from the default fixture_store , including results_bag . session_results_df and module_results_df are the dataframe equivalents of session_results_dct and module_results_dct The documentation has been updated so that users can get started more quickly by leveraging them. In addition: get_session_synthesis_dct can now take both a session or a request input. If a request is provided, the status of current item will be marked as 'pending', while not started items will be marked as 'unknown'. fixed bug in get_session_synthesis_dct : fixture parameters and saved fixtures where overriding each other in the final dict in flatten=True mode. Now fixture parameters appear as '<fixture_name>_param' . Fixed #15 . @saved_fixture can now be used without arguments. By default it will store fixtures in the default session-scoped 'fixture_store' fixture. HARVEST_PREFIX moved to common.py and is now exported at package level.","title":"1.1.0 - New default fixtures + fixture parameter names fix"},{"location":"changelog/#101-ordering-bug-fix","text":"Fixed pytest ordering issue, by relying on place_as . See #18","title":"1.0.1 - Ordering bug fix"},{"location":"changelog/#100-new-methods-for-pytest-session-analysis","text":"New methods are provided to analyse pytest session results: - filter_session_items(session, filter=None) is the filtering method used behind several functions in this package - it can be used independently. pytest_item_matches_filter is the inner method used to test if a single item matches the filter. - get_all_pytest_param_names(session, filter=None, filter_incomplete=False) lists all unique parameter names used in pytest session items, with optional filtering capabilities. Fixes #12 - is_pytest_incomplete(item) , get_pytest_status(item) , get_pytest_param_names(item) and get_pytest_params(item) allow users to analyse a specific item.","title":"1.0.0 - new methods for pytest session analysis"},{"location":"changelog/#090-get_session_synthesis_dct-filter-bugfix-test-id-formatter","text":"get_session_synthesis_dct : filter now correctly handles class methods. Fixed #11 new test_id_format option to process test ids. Fixed #9","title":"0.9.0 - get_session_synthesis_dct: filter bugfix + test id formatter"},{"location":"changelog/#080-documentation-better-filters-in-get_session_synthesis_dct","text":"Documentation: added a section about creating the synthesis table from inside a test function (fixes #4 ). Also, added a link to a complete example file. get_session_synthesis_dct : filter argument can now contain module names (fixed #7 ). Also now the function filters out incomplete tests by default. A new filter_incomplete argument can be used to display them again (fixed #8 ).","title":"0.8.0 - Documentation + better filters in get_session_synthesis_dct"},{"location":"changelog/#070-documentation-get_session_synthesis_dct-improvements-2","text":"Results bags do not measure execution time anymore since this is much less accurate than pytest duration. Fixes #6 get_session_synthesis_dct does not output the stage by stage details (setup/call/teardown) anymore by default, but a new option status_details allows users to enable them. Fixes #5 get_session_synthesis_dct has also 2 new options durations_in_ms and pytest_prefix to better control the output. Improved documentation.","title":"0.7.0 - Documentation + get_session_synthesis_dct improvements 2"},{"location":"changelog/#060-get_session_synthesis_dct-improvements","text":"get_session_synthesis_dct now has a test object filter , a flatten option, and and can now take optional storage objects as input to create a fully merged dictionary. See help(get_session_synthesis_dct) for details. Fixes #3 .","title":"0.6.0 - get_session_synthesis_dct improvements"},{"location":"changelog/#051-fixed-bug-with-pytest-2x","text":"Fixed #2 .","title":"0.5.1 - Fixed bug with pytest 2.x"},{"location":"changelog/#050-first-public-version","text":"First version validated against the data science benchmark pattern (yet to be published) get_session_synthesis_dct method to collect various test information already available @saved_fixture decorator supports both a variable or a fixture for the storage create_results_bag_fixture to create results bags Documentation","title":"0.5.0 - First public version"},{"location":"long_description/","text":"pytest-harvest \u00b6 Store data created during your pytest tests execution, and retrieve it at the end of the session, e.g. for applicative benchmarking purposes. The documentation for users is available here: https://smarie.github.io/python-pytest-harvest/ A readme for developers is available here: https://github.com/smarie/python-pytest-harvest","title":"pytest-harvest"},{"location":"long_description/#pytest-harvest","text":"Store data created during your pytest tests execution, and retrieve it at the end of the session, e.g. for applicative benchmarking purposes. The documentation for users is available here: https://smarie.github.io/python-pytest-harvest/ A readme for developers is available here: https://github.com/smarie/python-pytest-harvest","title":"pytest-harvest"}]}